---
title: "Literature prediction"
author: "Juho Salminen"
date: "24 Jul 2015"
output: html_document
---

```{r, echo=FALSE}
library(knitr)
opts_chunk$set(echo = FALSE, message = FALSE)
```


```{r,echo=FALSE, message=FALSE, results='hide', warning=FALSE}
# Loading libraries
library(ggplot2)
library(splitstackshape)
library(igraph)
library(tm)
library(RWeka)
library(randomForest)
library(knitr)
library(caret)
library(RColorBrewer)
library(wordcloud)
library(ROCR)

# Set ggplot theme
theme_set(theme_minimal(15))
startTime <- Sys.time()
```


```{r, echo=FALSE, warning=FALSE, results='hide'}
# Loading and preparing data

# Call cleaning2.R to process the data in the input folder and 
# save processed files to output folder
source("cleaning2.R", chdir = T)

# Helper function to remove leading and trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

# Fixing variable types
literature$AuthorFullName <- as.character(literature$AuthorFullName)
literatureByAuthor$AuthorFullName <- as.character(literatureByAuthor$AuthorFullName)
literatureByKeywords$AuthorFullName <- as.character(literatureByKeywords$AuthorFullName)
literatureByCategory$AuthorFullName <- as.character(literatureByCategory$AuthorFullName)

literature$Abstract <- as.character(literature$Abstract)
literature$DocumentTitle <- as.character(literature$DocumentTitle)

literature$YearPublished <- as.numeric(as.character(literature$YearPublished))

literature$CitedReferences <- as.character(literature$CitedReferences)

literature$TimesCited <- as.numeric(literature$TimesCited)
literatureByAuthor$TimesCited <- as.numeric(literatureByAuthor$TimesCited)
literatureByKeywords$TimesCited <- as.numeric(literatureByKeywords$TimesCited)
literatureByCategory$TimesCited <- as.numeric(literatureByCategory$TimesCited)

literature$AuthorKeywords <- as.character(literature$AuthorKeywords)

literatureByKeywords$AuthorKeywords <- as.character(literatureByKeywords$AuthorKeywords)
```


```{r, echo=FALSE, warning=FALSE}
# Create igraph
citationGraph <- graph.data.frame(citationEdges, vertices = citationNodes)
# Calculate PageRanks
citationNodes$PageRank <- page.rank(citationGraph)$vector
# Calculate in-degrees
citationNodes$InDegree <- degree(citationGraph, mode = "in")

# Merge to literature NEW
citationsLit <- citationNodes[citationNodes$Origin == "literature", ]
literature <- merge(literature, citationsLit[, c(1,3:7)],  
                      by.y = "FullReference", by.x = "ReferenceString")

# Create categorizations / evaluation metrics
current_year <- as.numeric(format(Sys.Date(), "%Y"))
literature$Age <- current_year - literature$YearPublished
literature$CitationsPerYear <- literature$TimesCited / literature$Age

# Median citations
medianCitation <- median(literature$TimesCited, na.rm = T)
literature$IsCited <- 0
literature$IsCited[literature$TimesCited > medianCitation] <- 1
literature$IsCited <- as.factor(literature$IsCited)
```


```{r, echo=FALSE, message=FALSE}
# Text mining

# Extract abstracts 
abstracts <- literature$Abstract
# Create corpus
abstractCorpus <- Corpus(VectorSource(abstracts))

# Clean up
abstractCorpus <- tm_map(abstractCorpus, content_transformer(tolower), mc.cores=1)
abstractCorpus <- tm_map(abstractCorpus, removePunctuation, mc.cores = 1)
abstractCorpus <- tm_map(abstractCorpus, stripWhitespace)
abstractCorpus <- tm_map(abstractCorpus, function(x) removeWords(x, stopwords("english")),
                         mc.cores=1)

# Stem corpus
abstractCorpus <- tm_map(abstractCorpus, stemDocument, mc.cores = 1)

# Create tokenizer
tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 3))
# Create n-gram document-term matrix
options(mc.cores=1)
abstractDTM <- DocumentTermMatrix(abstractCorpus, control = list(tokenize = tokenizer))

# Remove sparse terms
abstractDTM <- removeSparseTerms(abstractDTM, sparse = 0.75)
# Convert to data frame
abstractDF <- as.data.frame(as.matrix(abstractDTM))
names(abstractDF) <- paste("ABS", names(abstractDF), sep = "_")

# Extract titles
titles <- literature$DocumentTitle
# Create corpus
titleCorpus <- Corpus(VectorSource(titles))

# Clean up
titleCorpus <- tm_map(titleCorpus, content_transformer(tolower), mc.cores=1)
titleCorpus <- tm_map(titleCorpus, removePunctuation, mc.cores = 1)
titleCorpus <- tm_map(titleCorpus, stripWhitespace)
titleCorpus <- tm_map(titleCorpus, function(x) removeWords(x, stopwords("english")),
                         mc.cores=1)

# Stem corpus
titleCorpus <- tm_map(titleCorpus, stemDocument, mc.cores = 1)
# Create term document matrix
titleDTM <- DocumentTermMatrix(titleCorpus)
# Remove sparse terms
titleDTM <- removeSparseTerms(titleDTM, sparse = 0.75)
# Convert to data frame
titleDF <- as.data.frame(as.matrix(titleDTM))
# Fix column names
names(titleDF) <- paste("TITLE", names(titleDF), sep = "_")
```

# Titles wordcloud
```{r, echo=FALSE}
# Create wordcloud
pal2 <- brewer.pal(8, "Dark2")
wordcloud(titleCorpus, min.freq = 10, max.words = 100, random.order = F, 
          colors = pal2, scale = c(3, .5))
```


# Abstracts wordcloud
```{r, echo=FALSE}
# Create wordcloud
pal2 <- brewer.pal(8, "Dark2")
wordcloud(abstractCorpus, min.freq = 10, max.words = 100, random.order = F, 
          colors = pal2, scale = c(3, .5))
```


```{r, echo=FALSE}
# Extract features
literature2 <- literature[, c(2, 3, 8, 22, 33, 57, 59, 68, 70)]
literature2$PublicationType <- as.factor(literature2$PublicationType)

# Author names
literature2 <- cSplit_e(literature2, "AuthorFullName", sep = ";", 
                    type = "character", fill = 0)

# Author keywords
literature2 <- cSplit_e(literature2, "AuthorKeywords", sep = ";", 
                           type = "character", fill = 0)

# Subject category
literature2 <- cSplit_e(literature2, "SubjectCategory", sep = ";", 
                    type = "character", fill = 0)

# Merge to literature
literature2 <- cbind(literature2, abstractDF, titleDF)

# Publications
if (length(levels(literature$PublicationName)) > 1) {
  publications <- as.data.frame(predict(
    dummyVars(~ PublicationName, data = literature), newdata = literature))
  literature2 <- cbind(literature2, publications)
}

# Conference title
if (length(levels(literature$ConferenceTitle)) > 1) {
  conferences <- as.data.frame(predict(
    dummyVars(~ ConferenceTitle, data = literature), newdata = literature))
  literature2 <- cbind(literature2, conferences)
}

# Remove dummy columns with only one occurence
sums <- colSums(literature2[ , -c(1:9)] != 0) > 1
columns <- names(sums[sums == TRUE])
literature2 <- literature2[ , c(names(literature2)[1:9], columns)]

# Fix column names
lit_names <- names(literature2)
lit_names <- gsub("[^[:alnum:]]", "", lit_names)
ids <- c(1:length(lit_names))
lit_names <- paste(lit_names, ids, sep = "_")
names(literature2) <- lit_names

# Separate old and new literature
new_lit <- literature2[literature2$Age <= 1, ]
old_lit <- literature2[literature2$Age > 1, ]
```

```{r, echo=FALSE}
if (nrow(old_lit) > 0 & nrow(new_lit) > 0) {
  # Timing the algorithm
  stopTime <- Sys.time()
  cat("Munging time: ", stopTime - startTime)
  startTime <- Sys.time()
  # Final model
  old_lit <- old_lit[complete.cases((old_lit)), ]
  model_3 <- randomForest(IsCited_9 ~., data = old_lit[, -c(1, 3, 4, 7, 8)], 
                        ntree = 201)
  new_lit$Prediction <- predict(model_3, newdata = new_lit, type = "prob")[, 2]
  endTime <- Sys.time()
  cat("Execution time:", endTime - startTime, "\n")
  cat("Features:", ncol(old_lit), "\n")
  cat("Training set:", nrow(old_lit), "\n")
  cat("Test set:", nrow(new_lit))
}
```

## Baseline vs. model
```{r, echo=FALSE}
if (exists("model_3")) {
  table(literature$IsCited) / nrow(literature)
  model_3$confusion
}
```


## Variable importance
```{r, echo=FALSE, fig.height=6, fig.width=10}
if (exists("model_3")) {
  varImpPlot(model_3, type=2, n.var=30, scale=FALSE, 
           main="Top 30 variable Importance (Gini)")
}
```

## Positive error rate vs. number of trees
```{r, echo=FALSE}
if (exists("model_3")) {
  error_rate <- as.data.frame(model_3$err.rate)
  names(error_rate) <- c("OOB", "zero", "one")
  ggplot(error_rate, aes(c(1:model_3$ntree), one)) + geom_line() + 
    xlab("Number of trees") + ylab("Positive error rate")
}
```


## ROC
```{r, echo=FALSE}
if (exists("model_3")) {
  pred <- prediction(model_3$votes[, 2], old_lit$IsCited_9)

  perf <- performance(pred, measure="tpr", x.measure="fpr")
  plot(perf, col=rainbow(10))
  abline(0,1, lty=2)
}
```

## Cutoffs vs. precision
```{r, echo=FALSE}
if (exists("model_3")) {
  perf <- performance(pred, measure="prec")
plot(perf, col=rainbow(10))
}

if (exists("model_3")) {
  # Extract the articles included in the data set and articles not included
  # in the dataset
  citationsLit <- citationNodes[citationNodes$Origin == "literature", ]
  citationsLit <- merge(citationsLit, literature[, c("id", 
                                                     "ReferenceString", 
                                                     "DocumentTitle",
                                                     "Abstract", 
                                                     "TimesCited",
                                                     "DOI")], 
                         by.x = "FullReference", by.y = "ReferenceString")
  citationsLit$Article <- paste(toupper(citationsLit$DocumentTitle), " | ",
                                citationsLit$FullReference, " | ", 
                                        citationsLit$Abstract)
  citationsRef <- citationNodes[citationNodes$Origin == "reference", ]

  # Trim FullReference to 100 characters
  citationsLit$FullReference <- strtrim(citationsLit$FullReference, 100)
  citationsRef$FullReference <- strtrim(citationsRef$FullReference, 100)

  # Merge predictions
  citationsLit <- merge(citationsLit, new_lit[, c("id_1", "Prediction")], 
                        by.x = "id", by.y = "id_1", all = TRUE)
  # Extract papers to be printed
  citationsLit <- citationsLit[with (citationsLit, order(-TimesCited)), ]
  topLit <- head(citationsLit, 25)
  citationsLit <- citationsLit[with (citationsLit, order(-InDegree)), ]
  topLit <- rbind(topLit, head(citationsLit, 25))
  citationsLit <- citationsLit[with (citationsLit, order(-PageRank)), ]
  topLit <- rbind(topLit, head(citationsLit, 25))
  predictedLit <- citationsLit[with (citationsLit, order(-Prediction)), ]
  topLit <- rbind(topLit, head(predictedLit, 25))

  #topLit <- head(predictedLit, 25)
  predictedLit <- citationsLit[with (citationsLit, 
                                     order(-Prediction, -InDegree, -TimesCited,
                                           -PageRank)), ]
  predictedLit <- predictedLit[!is.na(predictedLit$Prediction), ]
  predictedLit$Prediction <- round(predictedLit$Prediction, 2)
  predictedLit$Prediction[is.na(predictedLit$Prediction)] <- ""
  predictedLit$Ranking <- c(1:nrow(predictedLit))

  #topLit <- topLit[!is.na(topLit$id), ]
}
```

# Most important papers
Prediction is a a predicted probability that a new paper (published during this year or the last year) will get more than `r medianCitation` (median) citations in the future. The prediction is based on a Random Forest classification model fit on the data from older papers (at least 2 years since publication).
```{r, echo=FALSE}
if (exists("model_3")) {
  kable(predictedLit[, c("Ranking", "Article", "InDegree", "TimesCited",
                         "PageRank", "Prediction")])
}
```

